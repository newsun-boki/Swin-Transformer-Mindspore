from mindspore.common.api import _MindSporeFunction
import numpy as np

from mindspore.common.initializer import HeUniform, initializer
import mindspore.nn as nn
import mindspore
from mindspore.ops import operations as P
from mindspore.ops import functional as F
from mindspore.ops import composite as C
from mindspore.common.tensor import Tensor
from mindspore.common.parameter import Parameter

from mindspore.common import dtype as mstype
from mindspore.nn.wrap.grad_reducer import DistributedGradReducer
from mindspore.communication.management import get_group_size
from mindspore.context import ParallelMode
from mindspore import context
import math
import warnings
from numpy.core.fromnumeric import reshape, transpose
from numpy.lib.shape_base import expand_dims
from scipy import special

#copy from timm

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                    "The distribution of values may be incorrect.",
                    stacklevel=2)
    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)
    tensor = Tensor(special.erfinv(np.random.uniform(2*l - 1,2*u - 1, tensor.shape)))
    mul = P.Mul()
    tensor = mul(tensor,std * math.sqrt(2.))
    add = P.Add()
    tensor = add(tensor,mean)
    min_value = Tensor(a, mindspore.float32)
    max_value = Tensor(b, mindspore.float32)
    tensor = C.clip_by_value(tensor,min_value,max_value)
    return tensor



def drop_path(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.

    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + Tensor(np.random.rand(shape[0])).astype(x.dtype)
    floor = P.Floor()
    random_tensor = floor(random_tensor)  # binarize
    div = P.Div()
    output = div(x,keep_prob) * random_tensor
    return output


class DropPath(nn.Cell):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def construct(self, x):
        return drop_path(x, self.drop_prob, self.training)

class MaskedFill(nn.Cell):
    r"""implement of torch.Tensor.masked_fill by mindspore
    Args:
        value: Value to replace the input Tensor
    """
    def __init__(self):
        super().__init__()
        self.minusend = Tensor([1],mindspore.float32)
        self.mul = P.Mul()
        self.sub = P.Sub()
    def construct(self, inputs:Tensor, mask:Tensor,value:float):
        masked = self.sub(self.minusend,mask)
        adder =value * mask
        inputs= self.mul(inputs,masked)
        outputs =inputs + adder
        return outputs
        

class Roll(nn.Cell):
    r"""simply implement of torch.roll by mindspore,only support dim=0,1,2
    """
    def __init__(self):
        super().__init__()
    def construct(self, x:Tensor, shifts:int, dim:int):
        if shifts > 0:
            if dim == 0:
                for shift in range(shifts):
                    tmp = x[-1]
                    for i in range(x.shape[0]-1,0,-1):
                        x[i] = x[i-1]
                    x[0] = tmp
            elif dim == 1:
                for shift in range(shifts):
                    for dim0 in range(x.shape[0]):
                        x_1 = x[dim0]
                        tmp = x_1[-1]
                        for i in range(x_1.shape[0]-1,0,-1):
                            x_1[i] = x_1[i-1]
                        x_1[0] = tmp
                        x[dim0] = x_1
            elif dim == 2:
                for shift in range(shifts):
                    for dim0 in range(x.shape[0]):
                        x_1 = x[dim0]
                        for dim1 in range(x_1.shape[0]):
                            x_2 = x_1[dim1]
                            tmp = x_2[-1]
                            for i in range(x_2.shape[0]-1,0,-1):
                                x_2[i] = x_2[i-1]
                            x_2[0] = tmp
                            x_1[dim1] = x_2
                        x[dim0] = x_1
        elif shifts < 0:
            shifts = -shifts
            if dim == 0:
                for shift in range(shifts):
                    tmp = x[0]
                    for i in range(0,x.shape[0]-1):
                        x[i] = x[i + 1]
                    x[-1] = tmp
            elif dim == 1:
                for shift in range(shifts):
                    for dim0 in range(x.shape[0]):
                        x_1 = x[dim0]
                        tmp = x_1[0]
                        for i in range(0,x_1.shape[0]-1):
                            x_1[i] = x_1[i+1]
                        x_1[-1] = tmp
                        x[dim0] = x_1
            elif dim == 2:
                for shift in range(shifts):
                    for dim0 in range(x.shape[0]):
                        x_1 = x[dim0]
                        for dim1 in range(x_1.shape[0]):
                            x_2 = x_1[dim1]
                            tmp = x_2[0]
                            for i in range(0,x_2.shape[0]-1):
                                x_2[i] = x_2[i-1]
                            x_2[-1] = tmp
                            x_1[dim1] = x_2
                        x[dim0] = x_1
        return x

class AdaptiveAvgPool1d_1(nn.Cell):
    def __init__(self):
        super().__init__()
    def construct(self, x):
        mean = P.ReduceMean(keep_dims=True)
        x = mean(x,-1)
        return x

def init_constant(tensor:Tensor,value):
    fill = P.Fill()
    return fill(tensor.dtype,tensor.shape,value)
